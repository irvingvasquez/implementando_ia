{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacion de U-NET\n",
    "@juan1rving\n",
    "\n",
    "En este notebook implemento la red conocida como Unet, la cual es utiliza para realizar segmentaci칩n sem치ntica. \n",
    "\n",
    "![Drag Racing](unet.png)\n",
    "\n",
    "El paper original es:\n",
    "\n",
    "> Ronneberger, O., Fischer, P., & Brox, T. (2015, October). U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta funcion lleva a cabo la doble convoluci칩n que se utiliza en el paper original. \n",
    "# En cada nivel se implementa.\n",
    "\n",
    "def double_conv(input_c, output_c):\n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(input_c, output_c, kernel_size = 3),\n",
    "        nn.ReLU(inplace = True),\n",
    "        nn.Conv2d(output_c, output_c, kernel_size = 3),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "    return conv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corta el contorno del sensor para que se pueda concatenar a la convoluci칩n transpuesta\n",
    "\n",
    "def crop_tensor(input_tensor, target_tensor):\n",
    "    target_size = target_tensor.size()[2]\n",
    "    input_size = input_tensor.size()[2]\n",
    "    delta = input_size - target_size # assuming that input is larger\n",
    "    delta = delta // 2\n",
    "    return input_tensor[:, :, delta:input_size-delta, delta:input_size-delta]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementacion de UNET \n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Maxpool reutilizable\n",
    "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        # Encoder\n",
    "        self.down_conv_1 = double_conv(1, 64)\n",
    "        self.down_conv_2 = double_conv(64, 128)\n",
    "        self.down_conv_3 = double_conv(128, 256)\n",
    "        self.down_conv_4 = double_conv(256, 512)\n",
    "        self.down_conv_5 = double_conv(512, 1024)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up_tconv_5 = nn.ConvTranspose2d(in_channels = 1024, out_channels = 512, kernel_size = 2, stride = 2)\n",
    "        self.up_conv_4 = double_conv(1024,512)\n",
    "        \n",
    "        self.up_tconv_4 = nn.ConvTranspose2d(in_channels = 512, out_channels = 256, kernel_size = 2, stride = 2)\n",
    "        self.up_conv_3 = double_conv(512,256)\n",
    "        \n",
    "        self.up_tconv_3 = nn.ConvTranspose2d(in_channels = 256, out_channels = 128, kernel_size = 2, stride = 2)\n",
    "        self.up_conv_2 = double_conv(256,128)\n",
    "        \n",
    "        self.up_tconv_2 = nn.ConvTranspose2d(in_channels = 128, out_channels = 64, kernel_size = 2, stride = 2)\n",
    "        self.up_conv_1 = double_conv(128,64)\n",
    "        \n",
    "        # Conversion a 2 canales\n",
    "        self.out = nn.Conv2d(in_channels = 64, out_channels = 2, kernel_size = 1)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        # encoder\n",
    "        \n",
    "        x1 = self.down_conv_1(image)\n",
    "        print(x1.size())\n",
    "        x1_down = self.max_pool_2x2(x1)\n",
    "        print(x1_down.size())\n",
    "        \n",
    "        x2 = self.down_conv_2(x1_down)\n",
    "        x2_down = self.max_pool_2x2(x2)\n",
    "        print(x2_down.size())\n",
    "        \n",
    "        x3 = self.down_conv_3(x2_down)\n",
    "        x3_down = self.max_pool_2x2(x3)\n",
    "        print(x3_down.size())\n",
    "        \n",
    "        x4 = self.down_conv_4(x3_down)\n",
    "        x4_down = self.max_pool_2x2(x4)\n",
    "        print(x4_down.size())\n",
    "        \n",
    "        x5 = self.down_conv_5(x4_down)\n",
    "        print(x5.size())\n",
    "        \n",
    "        #decoder\n",
    "        y4 = self.up_tconv_5(x5)\n",
    "        print(y4.size())\n",
    "        crop4 = crop_tensor(x4, y4)\n",
    "\n",
    "        y4_up = self.up_conv_4(torch.cat([crop4, y4], 1))\n",
    "        print(y4_up.size())\n",
    "        \n",
    "        y3 = self.up_tconv_4(y4_up)\n",
    "        crop3 = crop_tensor(x3, y3)\n",
    "        y3_up = self.up_conv_3(torch.cat([crop3, y3],1) )\n",
    "        print(y3_up.size())\n",
    "                \n",
    "        y2 = self.up_tconv_3(y3_up)\n",
    "        crop2 = crop_tensor(x2, y2)\n",
    "        y2_up = self.up_conv_2(torch.cat([crop2, y2],1) )\n",
    "        print(y2_up.size())\n",
    "        \n",
    "        y1 = self.up_tconv_2(y2_up)\n",
    "        crop1 = crop_tensor(x1, y1)\n",
    "        \n",
    "        y1_fin = self.up_conv_1(torch.cat([crop1, y1],1) )\n",
    "        print(y1_fin.size())\n",
    "        \n",
    "        output =  self.out(y1_fin)\n",
    "        print(output.size())\n",
    "        \n",
    "        return output                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch channels y x\n",
    "image = torch.rand((1,1,572,572))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 568, 568])\n",
      "torch.Size([1, 64, 284, 284])\n",
      "torch.Size([1, 128, 140, 140])\n",
      "torch.Size([1, 256, 68, 68])\n",
      "torch.Size([1, 512, 32, 32])\n",
      "torch.Size([1, 1024, 28, 28])\n",
      "torch.Size([1, 512, 56, 56])\n",
      "torch.Size([1, 512, 52, 52])\n",
      "torch.Size([1, 256, 100, 100])\n",
      "torch.Size([1, 128, 196, 196])\n",
      "torch.Size([1, 64, 388, 388])\n",
      "torch.Size([1, 2, 388, 388])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1318, 0.1312, 0.1292,  ..., 0.1303, 0.1284, 0.1317],\n",
       "          [0.1297, 0.1334, 0.1307,  ..., 0.1280, 0.1305, 0.1279],\n",
       "          [0.1303, 0.1321, 0.1306,  ..., 0.1310, 0.1286, 0.1308],\n",
       "          ...,\n",
       "          [0.1303, 0.1313, 0.1310,  ..., 0.1317, 0.1285, 0.1292],\n",
       "          [0.1290, 0.1295, 0.1294,  ..., 0.1289, 0.1291, 0.1307],\n",
       "          [0.1309, 0.1271, 0.1318,  ..., 0.1280, 0.1293, 0.1331]],\n",
       "\n",
       "         [[0.0526, 0.0530, 0.0540,  ..., 0.0548, 0.0540, 0.0537],\n",
       "          [0.0544, 0.0560, 0.0537,  ..., 0.0538, 0.0501, 0.0560],\n",
       "          [0.0543, 0.0534, 0.0534,  ..., 0.0544, 0.0539, 0.0545],\n",
       "          ...,\n",
       "          [0.0547, 0.0533, 0.0518,  ..., 0.0516, 0.0506, 0.0539],\n",
       "          [0.0538, 0.0527, 0.0561,  ..., 0.0533, 0.0549, 0.0557],\n",
       "          [0.0543, 0.0513, 0.0568,  ..., 0.0518, 0.0493, 0.0518]]]],\n",
       "       grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
